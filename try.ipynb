{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import numpy\n",
    "from tensorflow import float32\n",
    "from tensorflow.keras import Input\n",
    "#from tensorflow.keras import Output\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "from tensorflow import losses\n",
    "from tensorflow import optimizers\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input of the Encoder. Represent a word as a vector of numbers.\n",
    "from model_word_encoder import Word\n",
    "#Output of the Encoder, Input of the decoder. Represent a word in a compressed vector space.\n",
    "from model_word_encoder import Encoded_word\n",
    "#Encoder. Word->Encoded_word\n",
    "from model_word_encoder import Encoder_word_to_encoded_word\n",
    "#Decoder. Encoded_word->Word\n",
    "from model_word_encoder import Decoder_encoded_word_to_word\n",
    "\n",
    "from model_word_encoder import Hourglass_encoder_decoder\n",
    "\n",
    "from train import get_training_files\n",
    "from train import get_training_data \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length: 32 | Word: [ 83 104  97 107  97   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      "shape:  (32,)\n",
      "len:  32\n"
     ]
    }
   ],
   "source": [
    "my_word = Word(\"Shaka\")\n",
    "print( my_word )\n",
    "print( \"shape: \", my_word.gln_word.shape )\n",
    "print( \"len: \", len( my_word.gln_word ) )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10)                330       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 341\n",
      "Trainable params: 341\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "my_model = Sequential()\n",
    "my_model.add( layers.Flatten(input_shape=(my_word.gn_digits,) ) )\n",
    "my_model.add( layers.Dense(10) )\n",
    "my_model.add( layers.Dense(1) )\n",
    "my_model.compile( loss = losses.MeanSquaredError(), optimizer = optimizers.Adam( clipnorm=1 ) )\n",
    "#my_model.compile(optimizer='adam', loss=loss_fn, metrics=['accuracy'])\n",
    "my_model.summary()\n",
    "#show the model\n",
    "#plot_model(my_model,to_file='encoder.png',show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 84099 words for a total of  characters\n",
      "length: 84099 | Word[0] Length: 32 | Word: [97  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "#download the database of words\n",
    "TRAINING_FOLDER = \"Training\"\n",
    "ls_training_files = get_training_files( TRAINING_FOLDER )\n",
    "ls_training_words = get_training_data( ls_training_files[0] )\n",
    "n_words = len( ls_training_words )\n",
    "\n",
    "#convert words str() -> Word\n",
    "lc_training_words = list()\n",
    "for s_word in ls_training_words:\n",
    "    lc_training_words.append( Word( s_word ) )\n",
    "print( f\"length: {len(lc_training_words)} | Word[0] {lc_training_words[0]}\" )\n",
    "\n",
    "#create a dummy output supervised vector\n",
    "lc_dummy_output = list()\n",
    "for x in range(n_words):\n",
    "    lc_dummy_output.append( Encoded_word() )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Length: 32 | Word: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Model: \"Encoder_W_to_eW\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_2 (Flatten)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 10)                330       \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 2)                 22        \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 10)                30        \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 32)                352       \n",
      "=================================================================\n",
      "Total params: 734\n",
      "Trainable params: 734\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"Encoder_W_to_eW\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_2 (Flatten)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 10)                330       \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 2)                 22        \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 10)                30        \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 32)                352       \n",
      "=================================================================\n",
      "Total params: 734\n",
      "Trainable params: 734\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Summary:  None\n",
      "Number of Words: 84099\n",
      "First Word: Length: 32 | Word: [97  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0]\n",
      "Last Word: Length: 32 | Word: [112 111 113 117 101   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      "Database: [[ 97   0   0 ...   0   0   0]\n",
      " [ 97  97   0 ...   0   0   0]\n",
      " [ 97  97  97 ...   0   0   0]\n",
      " ...\n",
      " [108  97 110 ...   0   0   0]\n",
      " [109 105 103 ...   0   0   0]\n",
      " [112 111 113 ...   0   0   0]]\n",
      "2629/2629 [==============================] - 4s 1ms/step - loss: 2027.6530\n",
      "[[7.9206284e+01 7.7897316e+01 7.8032944e+01 ... 1.8454915e-03\n",
      "  9.3052536e-04 1.1515990e-04]\n",
      " [7.9206284e+01 7.7897316e+01 7.8032944e+01 ... 1.8454915e-03\n",
      "  9.3052536e-04 1.1515990e-04]\n",
      " [7.9206284e+01 7.7897316e+01 7.8032944e+01 ... 1.8454915e-03\n",
      "  9.3052536e-04 1.1515990e-04]\n",
      " ...\n",
      " [7.9206291e+01 7.7897324e+01 7.8032944e+01 ... 1.8457145e-03\n",
      "  9.3056262e-04 1.1511892e-04]\n",
      " [7.9206291e+01 7.7897324e+01 7.8032944e+01 ... 1.8457145e-03\n",
      "  9.3056262e-04 1.1511892e-04]\n",
      " [7.9206291e+01 7.7897324e+01 7.8032944e+01 ... 1.8457145e-03\n",
      "  9.3056262e-04 1.1511892e-04]]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "set_word() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [9], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m c_hourglass \u001b[38;5;241m=\u001b[39m Hourglass_encoder_decoder()\n\u001b[0;32m      7\u001b[0m c_hourglass\u001b[38;5;241m.\u001b[39mbuild( Word(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m), Encoded_word() )\n\u001b[1;32m----> 8\u001b[0m lc_prediction \u001b[38;5;241m=\u001b[39m \u001b[43mc_hourglass\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43mlc_training_words\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Data\\Project\\Project Programming\\Project Python\\2022-09-09 Python Word Encoder Decoder\\model_word_encoder.py:302\u001b[0m, in \u001b[0;36mHourglass_encoder_decoder.train\u001b[1;34m(self, ilc_words)\u001b[0m\n\u001b[0;32m    300\u001b[0m \u001b[39m#Construct a list of Words\u001b[39;00m\n\u001b[0;32m    301\u001b[0m \u001b[39mprint\u001b[39m( lnn_prediction )\n\u001b[1;32m--> 302\u001b[0m ilc_predicted_words \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mset_list_word( lnn_prediction )\n\u001b[0;32m    303\u001b[0m \u001b[39mprint\u001b[39m( ilc_predicted_words )\n\u001b[0;32m    305\u001b[0m \u001b[39mreturn\u001b[39;00m ilc_predicted_words\n",
      "File \u001b[1;32mc:\\Data\\Project\\Project Programming\\Project Python\\2022-09-09 Python Word Encoder Decoder\\model_word_encoder.py:238\u001b[0m, in \u001b[0;36mHourglass_encoder_decoder.set_list_word\u001b[1;34m(self, inn_digits)\u001b[0m\n\u001b[0;32m    236\u001b[0m \u001b[39mfor\u001b[39;00m ln_word \u001b[39min\u001b[39;00m inn_digits[:,]:\n\u001b[0;32m    237\u001b[0m     c_word \u001b[39m=\u001b[39m Word(\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 238\u001b[0m     c_word\u001b[39m.\u001b[39;49mset_word( ln_word )\n\u001b[0;32m    239\u001b[0m     lc_word\u001b[39m.\u001b[39mappend( Word(\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m) )\n\u001b[0;32m    242\u001b[0m \u001b[39mreturn\u001b[39;00m lc_word\n",
      "\u001b[1;31mTypeError\u001b[0m: set_word() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "ln_dummy_input = numpy.zeros( (len(lc_training_words), 32), dtype=numpy.uint8 )\n",
    "for n_index_word, ln_word in enumerate(lc_training_words):\n",
    "    #print(f\"Iteration: {n_index_word} | {ln_word} \")\n",
    "    ln_dummy_input[n_index_word, :] = ln_word.gln_word\n",
    "\n",
    "c_hourglass = Hourglass_encoder_decoder()\n",
    "c_hourglass.build( Word(\"\"), Encoded_word() )\n",
    "lc_prediction = c_hourglass.train( lc_training_words )\n",
    "\n",
    "\n",
    "\n",
    "#ln_dummy_output = numpy.random.rand( 84099, 1 )\n",
    "\n",
    "\n",
    "\n",
    "#my_model.fit( ln_dummy_input, ln_dummy_output, epochs=1 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lc_prediction' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mlc_prediction\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'lc_prediction' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "173ae74ecd759d33659dc89cc0ace91dba90ddaa088b7a848a7f37d845ddcc5d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
